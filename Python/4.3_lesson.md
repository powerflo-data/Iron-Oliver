# Lesson 4.3: Feature Methods & Advanced Regular Expressions

### Lesson Duration: 3 hours

> Purpose: The purpose of this lesson is to explain the differences between _feature engineering_, _feature extraction_, and _feature selection_. The students will learn to apply feature extraction methods and we will also revisit some of the feature selection methods used in the previous classes. We will talk about advanced regular expressions.

---



### Lesson 1 key concepts

Advanced regular expressions

- Revisit regular expressions
- Using quantifiers
- Using other metacharacters

```python
import re
text = "That person wears marvelous trousers."
pattern = 'er'
pattern = '[er]'
re.findall(pattern, text)
```

```python
text = "Is it spelled gray or grey?"
# text = "Is it spelled gry or grey?"
pattern = 'gr[ae]y'
re.findall(pattern, text)
```

```python
text = '''TKerraPower, A nuclear-energy company founded by Bill Gates,
        is unlikely to follow through on building a demonstration reactor in China,
        due largely to the Trump administration�s crackdown on the country'''

pattern = '[A-Z][a-z]*'
# pattern = '[A-z]+'
pattern = '[A-Z][a-z]+'
print(re.findall(pattern, text))
```

- `\w`: Any alphanumeric character.
- `\W`: Any non-alphanumeric character.
- `\d`: Any numeric character.
- `\D`: Any non-numeric character.
- `\s`: Any whitespace characters.
- `\S`: Any non-whitespace characters.

```python
text = "If you tell the truth, you don't have to remember anything 100."

pattern = '\w'
pattern = '\w+'
pattern = '\w*'
# pattern = '\w?'
# pattern = '.'
print(re.findall(pattern, text))
```

```python
text = "If you tell the truth, you don't have to remember anything 100."
pattern = '\w{4}'
pattern = '\w{4,}'
print(re.findall(pattern, text))
```

```python
text = """
Aeromexico 800 -237- 6639
Air Canada 888-247-2262
Air Canada Rouge 888-247-2262
Air Creebec 800-567-6567
Air Inuit 800-361-2965
Air North 800-661-0407
Air Tindi 888-545-6794"""
pattern = '\d+-\d+-\d+'
re.findall(pattern, text)
```

### Lesson 2 key concepts

Introduction to feature engineering

- Feature extraction
- Feature selection

- **Feature engineering** is a process of transforming the given data into a form that is easier to interpret. All the techniques that we have used before including data transformations techniques, encoding categorical variables, scaling numerical features, imputing missing values, cleaning categorical columns, using regular expressions, using DateTime, and string functions are feature engineering techniques. The key idea is that we are manipulating the information that is available to us to be able to better understand it and improve the model.

- **Feature extraction/feature generation**: It is the process of extracting relevant information from the existing available information. For example, you are provided with the date of birth of customers in the data. You are not interested in when they were born but more interested in their age. In that case, you can use the DateTime functions to calculate the age or extract other relevant information such as the year they were born, the month they were born.

(WE WILL COVER FEATURE EXTRACTION HERE)

- **Feature selection**: This is the process of selecting the features/columns in your data that are relevant to the model, for eg. if it is a prediction problem, you will be more interested in only those variable that have an impact on your target and not other columns/features that are not providing any information about target. Adding variables that don't add value to the model degrades the performance of the model both in terms of accuracy and in terms of efficiency.

Some of the feature selection methods that we took a look at earlier include:

- Checking null values to drop a column
- Sense check to drop columns that are not significant
- Using heat maps to check multicollinearity for numerical variables
- Chi-square tests for categorical variables

Here we will use the column `DOB` to calculate the age of the client. As you can see, there are a lot of clients for whom the `DOB` is not known, so it is marked as 0. If we check, we will see that there are a lot of such clients so we can't just filter it out as we would lose a lot of data. So we would calculate the average age and later replace those **0**'s with mean age.

```python
# to check the number of clients for which this info is missing
len(data[data['DOB']==0])
```

- In the DOB column, the format is `YYMM`. We will use this info to get the year when they were born.

```python
import re
def year(x):
    x = str(x)
    if len(x)<4:
        return np.nan
    else:
        pattern = '\d\d'
        yr = re.findall(pattern,x)[0]
        return int(yr)
```

- Note that our reference year is _1997_ here as the data is from that study.

```python
data['year'] = list(map(year, data['DOB']))
data['year'] = 97 - data['year']

data['year'] = data['year'].fillna(np.mean(data['year']))

# Now we can drop the column DOB as we have extracted the information we need from this column
data = data.drop(['DOB'], axis=1)
```

### Lesson 3 key concepts

Revisiting feature selection techniques looked at earlier

- Checking null values to drop a column (discuss)
- Sense check to drop columns that are not significant (discuss)

- Only a very very brief introduction to reinforce feature selection. We have looked at these techniques earlier in-depth already.

- Using heat maps to check multicollinearity for numerical variables (in detail and as an introduction to statistical significance and p values)

Through multivariate linear regression, we are trying to assess the influence of each of the predictor variables on the target variable. This influence/relationship is linear and is represented by a mathematical equation. The equation is given as: `Y = β0 + β1X1 + β2X2 + β3X3 + β4X4 + ... + βnXn`.
Here, each variable is trying to explain some information about the nature of `Y`, how does `Y` change with each of the predictor variables. The change in `Y` with `X` is technically variation. When the predictor variables are all independent of each other, each variable explains some information on the change in `Y`. Multicollinearity arises when the predictor variables are highly correlated. Hence some predictors are redundant as they do not reveal any new information on the change in `Y` with a change in `X`.

The correlation matrix using the heat maps helps us understand the correlation between the independent variables.
Using the `sklearn.metrics` module we calculate `R` square statistic. It measures the proportion of variance in the dependent variable that is explained by all of the independent variables.

For checking multicollinearity, we calculate `R` square `k` and `VIF` (variance inflation factor) for each of the `k` independent variables. We do this by regressing the `k`-th independent variable on all of the other independent variables. That is, we treat `X` `k` as the dependent variable and use the other independent variables to predict `X` `k`.

For eg. `Y=β0+β1X1+β2X2+β3X3+β4X4`

Build a model `X1` vs. `X2 X3 X4`, find `R^2`, call it `R1`.
Build a model `X2` vs. `X1 X3 X4`, find `R^2`, call it `R2`.
and so on and so forth

**Interpreting R square k** - If `R2k `equals zero, variable `k` is not correlated with any other independent variable.
Usually, multicollinearity is a potential problem when `R2k` is greater than `0.75` and, a serious problem when `R2k` is greater than `0.9`.

For each variable that we find individual R2, `VIF = 1 / ( 1 - R2k )` (for each dependent variable `k`). It is used to assess multicollinearity.

**Interpretation of the variance inflation factor**: If `VIFk = 1`, variable k is not correlated with any other independent variable. Multicollinearity is a potential problem when `VIFk` is greater than 4 and, a serious problem when it is greater than 10.

### Effects of Multicollinearity:

1. It makes it harder to interpret the significance of variables in the regression model (we will talk about statistical significance/p-value later).
2. It might give good enough results due to over-fitting, but those will not be very reliable (we will talk about over-fitting and under-fitting in more detail later. Over-fitting for now, you can explain to the students as a more complicated model and not very generalized ie it might work on the data at hand very well but not so well on the unknown/out of the box data.
3. It is also important to note that it does not severely impact the model in terms of predicting power if the only prediction is the main goal of the analysis.

Dealing with High Multicollinearity:

1. Centering/standardizing/normalizing variables may help reduce multicollinearity.
2. Removing one or more of the variables that are highly correlated with each other.

### Lesson 4 key concepts

Implementing the `VIF` technique for feature selection.

```python
from statsmodels.stats.outliers_influence import variance_inflation_factor
from statsmodels.tools.tools import add_constant


# This is the code to show how to use the __variance_inflation_factor__ function
# We are using add_constant here as VIF method in python expects the addition of a constant terms in the X features. It uses OLS but does not add the constant itself.

vif = {}
data_corr = add_constant(data_corr)
for i in np.arange(data_corr.shape[1]):
    column_name = data_corr.columns[i]
    value = variance_inflation_factor(np.array(data_corr), i)
    vif[column_name] = value

# Code to use the variance_inflation_factor technique to remove highly correlated columns

flag = True
threshold = 50
data_corr = add_constant(data_corr)
while flag is True:
    #print(data_corr.head())
    flag = False
    values = [variance_inflation_factor(np.array(data_corr), i) for i in np.arange(data_corr.shape[1])]
    #print(values)
    if max(values)> threshold:
        col_index = values.index(max(values))
        column_name = data_corr.columns[col_index]
        data_corr = data_corr.drop([column_name], axis=1)
        flag = True
```

### Lab

- Link to the lab: [https://github.com/ironhack-labs/lab-feature-extraction](https://github.com/ironhack-labs/lab-feature-extraction)

### Additional Resources

[VIF_Application](https://stackoverflow.com/questions/42658379/variance-inflation-factor-in-python)
