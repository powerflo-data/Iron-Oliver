# Lesson 4.4: Feature Selection, Data Processing & KNN Algorithm

### Learning Objectives

- Perform feature selection for categorical variables
- Implement different scaling techniques for numerical variables
- Describe how KNN algorithm works

---

### Lesson 1 key concepts

Revisiting feature selection techniques looked at earlier

- Chi-square tests for independence of categorical variables

- Categorical variables: _nominal_ vs. _ordinal_ (a quick overview)
- _Chi-square test_ is used to determine if there is a significant relationship between two nominal (categorical) variables. It works with nominal categorical variables well and not well ordinal categorical variables as the test is based on a contingency table (as we would see later) and the order in which frequencies are put in the table, does not change the result of the chi-square test.

In the contingency table, the frequency of each category for one nominal variable is compared across the frequencies of categories of the second nominal variable. Here is the code to show how to check the contingency table in Python:

```python
# contingency table
data_crosstab = pd.crosstab(data['DOMAIN'], data['RFA_2'], margins = False)
data_crosstab
```

- Based on the data in the contingency table we calculate the expected value of the nominal variables. Based on the expected values, the _chi-square test_ statistic is calculated which helps us decide on whether the variables are independent or not. Technically, though the value of the test statistic we are trying to prove or disprove some hypotheses on the independence of categorical variables.

- `H0` (_Null Hypothesis_) - assumes that there is no association between the two variables.
- `Ha` (_Alternate Hypothesis_) - assumes that there is an association between the two variables.

### Lesson 2 key concepts

- Working with the established hypothesis

- If the observed chi-square test statistic is greater than the critical value (this value is known already based on certain parameters) in the data, the null hypothesis can be rejected.
- If the observed chi-square test statistic is lower than the critical value (this value is known already based on certain parameters) in the data, the null hypothesis is accepted (also put as we fail to reject the null hypothesis) ie. based on the statistics we either reject `H0` or we fail to reject `H0`. You can also use the `p` value directly as we will see later in the lesson.

- Three important values that we measure in order to calculate the Chi-square test statistic are:

      - Degrees of freedom `(r-1)\*(c-1)` where `r` is the number of rows and `c` is the number of columns
      - Actual frequencies
      - Expected frequencies

- Based on these values we calculate the test statistic that helps us determine if we reject or fail to reject the null hypothesis.

```python
from scipy.stats import chi2_contingency
chi2_contingency(data_crosstab, correction=False)
```

This returns 4 results in this order (_chi-square_ statistic, _p_ value, degrees of freedom, expected frequencies matrix). Looking at the _p_ value, it is usually compared against 0.05. We will talk about _p_ value later but now we will just use this to decide on the variables directly.

Since in this case, the _p_ value is less than 0.05 we can reject the null hypothesis (that there is no relationship between the two categorical variables); ie. there is a correlation between the two variables. Hence we can drop one of the two columns. In this case, we are going to drop the column `RFA_2`.

---

### Lesson 3 key concepts


- Data Preprocessing for numerical variables
- Difference between standardization, normalization and min-max scaling.

- _Standard scaler_: Removes the mean and scales the data to unit variance. For each column, each value in the column is subtracted by the mean of the column and then divided by the standard deviation.
- _Min-max scaler_: It scales the data in the range of `[0,1]`. For each column, each value in the column is subtracted by the max of the column and then divided by the difference of max and min of the column. It is very sensitive to the presence of outliers.
- _Normalize_: It rescales the vector to have a unit norm. This means that, for each column, each value is divided by the magnitude of the column. The magnitude is calculated as the euclidean distance.

```python
# We will use the data we got after using VIF step for feature selection
# we will use it for numerical variables

data_corr.head()
numerical = data_corr.drop(['AVGGIFT'], axis=1)
```

```python
# Standardization/Standard Scaler

from sklearn.preprocessing import StandardScaler
transformer = StandardScaler().fit(X_num)
x_standardized = transformer.transform(X_num)
x_standardized
```

```python
# Min-max scaler

from sklearn.preprocessing import MinMaxScaler
transformer = MinMaxScaler().fit(numerical)
x_min_max = transformer.transform(numerical)
x_min_max
```

```python
# Normalization

from sklearn.preprocessing import Normalizer
transformer = Normalizer().fit(numerical)
x_normalized = transformer.transform(numerical)
x_normalized
```

---

### Lesson 4 key concepts

- Processing categorical column `DOMAIN`
- Encoding categorical column `DOMAIN` variables
- Introduce the `KNN` algorithm

Keep using the same dataset we use in class.

```python
# Cleaning categorical column DOMAIN

vals_domain = pd.DataFrame(data['DOMAIN'].value_counts())
vals_domain = vals_domain.reset_index()
vals_domain.columns = ['domain', 'counts']
group_vals_domain_df = vals_domain[vals_domain['counts']<5000]
group_vals_domain = list(group_vals_domain_df['domain'])
group_vals_domain
```

```python
def clean_vals_domain(x):
    if x in group_vals_domain:
        return 'other'
    else:
        return x

data['DOMAIN'] = list(map(clean_vals_domain, data['DOMAIN']))
```

```python
from sklearn.preprocessing import OneHotEncoder
encoder = OneHotEncoder(handle_unknown='error', drop='first').fit(data[['DOMAIN']])
encoded = encoder.transform(data[['DOMAIN']]).toarray()
encoded
```

### :pencil2: Practice on key concepts - Lab: Normalization and encoding

- Link to the lab: [https://github.com/ironhack-labs/lab-data-cleaning-and-wrangling](https://github.com/ironhack-labs/lab-data-cleaning-and-wrangling)

### Additional Resources

- [Difference between `standardscaler` and Normalizer in `sklearn.preprocessing`](https://stackoverflow.com/questions/39120942/difference-between-standardscaler-and-normalizer-in-sklearn-preprocessing)
